Objective: To implement an automated system that generates and iteratively refines Python code to achieve a high score in a Kaggle competition. This document details the specific algorithms, models, and workflows required for a faithful recreation.

1. System Architecture
The system consists of a Core Generation Loop and a Post-Hoc Analysis Module.

Core Generation Loop: An iterative process responsible for creating and evaluating code.

TS Controller: Selects a parent code solution from the solution tree.

LLM Rewriter: Constructs a detailed prompt and uses an LLM to generate a new child code.

Sandbox: Executes the child code against the competition data.

Scorer: Evaluates the output and assigns a score.

The new code, score, and logs are added as a new node in the tree managed by the Controller.

Post-Hoc Analysis Module (Optional but Recommended): A set of tools used after the generation loop is complete to understand the search process and the conceptual space of the generated solutions. This module is not part of the search algorithm itself.

2. Core Component Implementation Details
2.1. The LLM Rewriter
Models:

Code Generation: A powerful instruction-following LLM (e.g., Gemini Pro series).

Idea Synthesis (for Recombination): A fast and capable model (e.g., Gemini 2.5 Flash) is used for summarizing and combining ideas.

Function: To perform intelligent code "mutation" by rewriting. The quality of the prompt is paramount.

Detailed Prompt Engineering: The prompt is a structured concatenation of multiple information blocks.

Context Block: The Kaggle competition description, data schema, and the exact evaluation metric.

Parent Node Block: The complete Python script from the selected parent node.

Feedback Block: The score achieved by the parent code, along with key lines from its stdout and stderr logs. This provides the LLM with context on why the previous code scored the way it did (e.g., errors, warnings, convergence logs).

Instruction Block: A clear instruction for the LLM. This is the most dynamic part:

Simple Improvement: "Improve the following code to get a better score. The previous score was [score]."

Research Idea Injection: "Implement the following method described in this research summary: [text summary of a paper or technique]. Adapt the provided code to use this approach."

Recombination Instruction: "You are an expert data scientist. The following two solutions approach the problem differently. Combine their strengths to create a new, superior hybrid solution: [recombination summary from Gemini 2.5 Flash]."

2.2. Code Execution Sandbox
Environment: A secure, containerized environment (e.g., Docker) with pre-installed libraries (pandas, scikit-learn, xgboost, etc.).

Error and Timeout Handling (Crucial Detail):

The sandbox must enforce a strict time limit per execution.

Code that crashes (raises an exception), times out, or produces a malformed submission.csv must be handled gracefully.

Such failed runs are not discarded; they are added to the tree with the worst possible score (e.g., -Infinity or a predefined penalty value). This teaches the search algorithm to avoid unproductive paths.

2.3. Scorer
Function: Implements the TaskScore(u) function from the algorithm.

Input: Predictions generated by the sandboxed code on a fixed, local validation set.

Output: A single floating-point score. The direction of the score must be consistent (higher is always better). For metrics where lower is better (e.g., MSE, WIS), the score should be negated (e.g., score = -MSE).

2.4. Tree Search (TS) Controller
Algorithm: PUCT, as described in the paper.

Key Algorithm Modifications (Critical Differences from AlphaZero):

Flat Search Selection: The selection step (argmax u in T) is performed over the entire set of existing nodes in the tree. This is not a recursive MCTS-style descent from the root. Every generated solution is a candidate for expansion on every iteration.

No Rollouts: The score of a node is determined by a single, direct execution of its code. There are no simulated playouts or probabilistic estimations of a node's future value. The "randomness" in scoring a prompt comes from the LLM's varied outputs, not from rollouts.

Data Structure: Each node u in the tree T stores: code, parent, TaskScore(u), V(u) (visit count), and logs (from execution).

Implementation of Selection Formula:

u 
∗
 = 
u∈T
argmax
​
 (RankScore 
T
​
 (u)+c 
puct
​
 P 
T
​
 (u) 
1+V(u)
N 
total
​
 

​
 
​
 )
The rank normalization of scores (RankScore) is a key feature to make the exploration constant c_puct more stable across different problems with varying score scales.

3. Workflow and Configuration
Configuration: Before starting, define key parameters:

max_nodes: The total number of solutions to generate (e.g., typically between 500 and 2000, based on the paper's experiments).

c_puct: The exploration constant for the PUCT formula.

Validation Split: Define and fix the local validation dataset.

Initialization (Root Node): As previously described, start with a simple baseline script to create the first node.

Execution Loop: Run the core generation loop until max_nodes is reached.

Final Solution Selection: The final output is the code from the node in the entire tree with the highest raw TaskScore(u).

4. Advanced Strategies: Seeding the Search with Research Ideas
This is how the system moves from basic optimization to expert-level problem-solving.

AI-Powered Ideation: Use external LLM-based tools (e.g., "Gemini Deep Research," "AI co-scientist") to generate an initial list of 5-10 distinct, high-level strategies for the Kaggle problem. Each strategy is formatted into a text summary.

Seeding the Tree: Instead of a single root node, run several smaller, independent tree searches, each initialized with one of the AI-generated research ideas in its initial prompt.

Automated Idea Recombination:

After an initial phase of the search (e.g., after 200 nodes have been generated), identify the top-performing nodes from conceptually different branches.

For pairs of these nodes (u 
A
​
 , u 
B
​
 ), use an LLM (Gemini 2.5 Flash) with a prompt like: "Analyze the core technical differences between code A and code B. Propose a new hybrid method combining their strengths."

Take the text output and use it as an "Instruction Block" in a new prompt, seeded with the code from the better of the two parents (u 
A
​
  or u 
B
​
 ). This launches a new, promising branch in the search tree.

5. Post-Hoc Analysis using Embeddings
This module is used to understand the results after the search is complete.

Code Embedding:

For each node u in the final tree, take its code snippet.

Input the code into a text embedding model (e.g., Gemini text embedding model) to get a high-dimensional vector representation (e.g., 3072 dimensions).

Purpose and Application:

Visualize Solution Space: Use dimensionality reduction techniques (e.g., UMAP, t-SNE) on the embedding vectors to create a 2D map of all generated solutions. This can reveal clusters of similar approaches (e.g., a "tree-based model" cluster, a "neural network" cluster).

Measure Novelty: Calculate the cosine similarity between a new node and its parent. A low similarity suggests a significant conceptual leap ("breakthrough").

Verify Strategy Adherence: For runs seeded with a specific research idea, check if the resulting embeddings are distinct from runs with other ideas, confirming the LLM followed instructions.